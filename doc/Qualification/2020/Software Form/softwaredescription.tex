%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NUbots' TDP 2020
%
% Date: 24.11.2019
%
%
\documentclass{llncs}
%
\usepackage{url}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{verbatim}
%
\begin{document}
%

\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{The NUbots Qualification Material for RoboCup 2020} % additional mark in the TOC
%
%
\mainmatter              % start of the contributions
%
\title{The NUbots Software Overview for RoboCup 2020}
%
\titlerunning{The NUbots Software Overview for RoboCup 2020}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used

\author{Matthew Amos
        \and Alex Biddulph
        \and Stephan Chalup
        \and Daniel Ginn
		\and Alexandre Mendes
        \and Timothy Mullen
        \and Josephus Paye
        \and Ysobel Sims
        \and Anita Sugo
        \and Peter Turner
        \and Taylor Young
        }
       
%
\authorrunning{Amos et al.}   % abbreviated author list (for running head)
%
%%%% modified list of auther for the TOC (add the affiliations)
\tocauthor{
M. Amos,
A. Biddulph,
S. Chalup,
D. Ginn,
A. Mendes,
T. Mullen,
J. Paye,
Y. Sims,
A. Sugo,
P. Turner,
T. Young
}

%
\institute{Newcastle Robotics Laboratory\\ School of Electrical Engineering and Computing\\
Faculty of Engineering and Built Environment\\
The University of Newcastle, Callaghan 2308, Australia\\
Contact: \email{nubots@newcastle.edu.au}\\
Homepage: \url{http://robots.newcastle.edu.au}}
%

\maketitle              % typeset the title of the contribution

\section{Walking}

% Please give a brief summary of your walking algorithm (max. 1000 characters).

\medskip

The current walk implementation uses an open-loop hybrid walk controller, dynamically switching between an analytically solved trajectory for the centre of mass based on the linear inverted pendulum model, and a Zero Moment Point preview controller, based on the work of Yi et al.~\cite{SeungJoon2013} and Song~\cite{Song2010DevelopmentOA}. To add local feedback, positions for the pitch and roll ankle joints are prescribed based on information from the Inertial Measurement Unit (IMU) to counteract small imbalances. 

Our current walk is an old walk used on the Darwin-OP robots and has major issues. Because of this we are investigating other walks to use for the 2020 competition. These include BitBot's Quintic Walk~\cite{bitbotsMotionGit} based off Rhoban's Quintic Walk~\cite{rhobanModelGit} and Rhoban's IK Walk~\cite{rhobanModelGit}. A quasi-static walk being developed as a mechatronics final year project is due to be completed by the 2020 competition. It uses a non-linear optimisation method to generate a quasi-static walk gait. A separate project involves developing a static walk using a dynamic foot and torso path planner. 

\section{Vision}

% Please give a brief summary of your vision algorithm, i.e. how your robots detect balls, field borders, field lines, goalposts and other robots (max. 1000 characters).

%%% Summary
% The Visual Mesh underpins the vision system, and is used for sparse detection of balls, points on the field, field lines, goal posts and other robots.
% From the Visual Mesh a series of specialised detectors are employed to detect field edges, balls, and goal posts. All points that the Visual Mesh has identified as either field points or field line points are clustered into connected regions and each cluster is then either merged or discarded using some heuristics until a single cluster remains, allowing an upper convex hull to be fitted.
% The ball detector finds all clusters which are below the upper convex hull. A circular cone is then fitted to each cluster. Different heuristics, such as degree of circle fit and different distance metrics, are then used to discard to cones.
% The goal post detector find all clusters which intersect the upper convex hull. Clusters are formed from goal post edge points and a determination of the bottom mid-point is made for each post.

\medskip

The Visual Mesh~\cite{Houliston2018VisualMR} underpins the vision system, and is used for sparse detection of balls, points on the field, field lines, goal posts and other robots.

The Visual Mesh is a convolutional neural network for object detection based on a mesh which samples pixels from an image. The mesh is depth independent and the number of mesh points is small enough to allow a CNN to run in real-time on a humanoid robot. Tests on the CPU from the Intel\textsuperscript{\textregistered} NUC7i7BNH show that a nine layer visual mesh had an execution time of 2.44ms~\cite{Houliston2018VisualMR}. Furthermore, the visual mesh does not degrade in accuracy when objects occur at different distances. This is due to the depth independence introduced by using the mesh. The visual mesh was used to detect a soccer ball on a field up to 10m away from the camera~\cite{Houliston2018VisualMR}. The visual mesh can detect soccer balls, field lines and goal posts, however it is built assuming the object to be detected is spherical.

The visual mesh is created using the following geometric assumptions for each image:
\begin{enumerate}
    \item The camera lens type, height and orientation with respect to the ground are known. 
    \item The object is spherical with known radius and remains on the ground.
\end{enumerate}
The height, orientation and radius are used to create a set of unit vectors based at the camera position. Any vector with origin at the camera can be thought of as a ray of light travelling towards the camera. If the vector is within the field of view of the camera it will be collected by the lens. The vector is mapped to a point on the image using the lens projection equation, provided the point is within the bounds of the image sensor of the camera. The visual mesh is formed by taking an array of vectors and associating each of them with a point and pixel in the image. The array of vectors is specially constructed using two equations to efficiently sample the space around the camera for the object. This means the points (sample points) on the image will efficiently sample the image for the object. Because the vectors are generated in the space around the camera, the sample is consistent despite changes in lens type, image resolution, and the apparent size of an object due to distance. Each sample point is connected to its six closest neighbours and these connections become the edges of the mesh. A fully convolutional neural network~\cite{ShelmarLong2017} is used on the mesh where each sample point and it's neighbours become the input to a convolution. All layers in the network use seven points convolutions since there are six neighbours for each sample point. A parameter controls the number of sample points on the object which determines the level of detail available to the network. 

The height and orientation of the camera is tracked using the kinematics and inertial measurement unit of the robot. The radius of the soccer ball is given. The unit vectors only need to be calculated once for each height and radius pair. As the robot walks its height varies. A series of visual meshes for different heights are calculated on startup. For each image this information is recorded and the appropriate visual mesh is chosen. A binary search pattern is used to select the vectors that will become points on the image based on the camera's orientation. The partitions of the binary search pattern are built on startup and the search is run in real-time. Every training and run-time image is converted into the mesh before the network is run. The network labels each point of the mesh with the probability it belongs to an object class. A post-processing algorithm based on the probabilities of the mesh can cluster the sample points to determine an object's position.

From the Visual Mesh a series of specialised detectors are employed to detect field edges, balls, and goal posts. Firstly, all points that the Visual Mesh has identified as either field points or field line points are clustered into connected regions and, each cluster is then either merged or discarded using some heuristics until a single cluster remains. Finally, an upper convex hull algorithm is applied to the final cluster determine the edge of the field.

The ball detector forms clusters out of all the points that the Visual Mesh has identified as ball points. Specifically, the clusters are formed from Visual Mesh points that are identified as being a ball point, but have at least $1$ neighbour which is \emph{not} a ball point. This allows us to form clusters of ball edge points. Any clusters which are not below the field edge are discarded. A circular cone is then fitted to each cluster. The cone axis is determined from the line segment between the centre on the camera and the average of all ball edge points. The radius of the cone is determined by the maximum distance between the ball edge points and the average of all of the ball edge points. Different heuristics, such as degree of circle fit and different distance metrics, are then used to discard to cones.

The goal post detector follows a similar structure to the ball detector. Clusters are formed from goal post edge points and any clusters that do not intersect the field edge are discarded. The bottom centre point of the goal post is then found by averaging the edge points. The distance to the goal posts are determined and if there are multiple goal posts detected an attempt is made to assign \emph{leftness} and \emph{rightness} to each post.

At present, a field line detector is not implemented, but will likely be implemented before the next competition.

All calculations in all detectors are done in 3D world coordinates.

\section{Localisation}

% Please give a brief summary of how your robots localize themselves on the field (max. 1000 characters).

The localisation on-board the robot is performed using a Particle Filter. This allows us to maintain multiple hypotheses about the current pose of the robot, providing robustness against the mirrored field problem and having multiple potential initial positions when entering the field. The filter relies on measurement updates from the vision module, and on IMU data for time updates. The measurement update previously only tracked the four goal post locations; however, we are working on introducing the tracking of field lines during 2020. In particular, we will be focusing on distinctive field features such as corners and the centre circle.

\medskip

\section{Behaviour}

% Please give a brief summary of your behavioral architecture and the decision processes of your robots (max. 1000 characters).

The robot's behaviour is a basic state machine where the robot will look for the ball and goals, position itself to kick the ball toward the goals, and then kick. The vision system determines which foot the robot will kick with. 

\medskip

\section{Contributions to RoboCup}

% List your previous participation (including rank) and contribution to the RoboCup community (release of open-source software, datasets, tools etc.)
% This description field is optional

The NUbots team participated in the 2019 Humanoid Teen-Size League and finished as quarter finalists. The NUbots have participated in the Four Legged League (2002-2007), the Standard Platform League (2008-2011), the Kid-Size Humanoid League (2012-2017), and the Teen-Size Humanoid league (2018-2019). NUbots were the Four Legged League world champions in 2006. The team won the first Standard Platform League in 2008 as team NUManoid in collaboration with the National University of Maynooth, Ireland.

The team has all its RoboCup code~\cite{nubotsGit}, hardware~\cite{nubotsHardwareGit}, and debugging tools~\cite{nubotsNUsightGit} developed as open source on GitHub. 

The NUbots team have developed a Blender plugin to generate semi-synthetic images with fully-annotated ground truth segmentation maps~\cite{nubotsNUpbrGit}. The images contain random ball positions, robot positions and kinematic poses, obstacles, and viewer orientations. This tool is public on GitHub for anyone in the League to use.

The NUbots team have been active participants in the Humanoid League rules discussions. A new standard communication protocol~\cite{nubotsProtocolGit}, based on Protobuf messages, was proposed by NUbots to the TC. A prototype tool, based on the NUsight~\cite{nubotsNUsightGit} debugging utility, for monitoring network communications and displaying robot communications in a meaningful manner is currently being developed.

A new development this year hopes to benefit not only the NUbots team but other teams interested in any of our systems. This is a comprehensive documentation resource in the form of a public website~\cite{nubotsNUbookGit}, providing detailed information about the hardware and software systems, as well as current and future projects. 


% Please list RoboCup-related papers your team published in 2019.
% This description field is optional

\bibliographystyle{plain}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{nubots}

\end{document}
